{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7h/uEuRGkhYdQj5LEseeV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konstihengge/Tensorflow_tutorial/blob/main/Tensorflow_tut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHpLe32vRxBC"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.version)\n",
        "\n",
        "t = tf.ones([5,5,5,5])\n",
        "\n",
        "t = tf.reshape(t, [125, -1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCdzp5JqkrX4"
      },
      "source": [
        "#Regression\n",
        "\n",
        "\n",
        "!pip install -q sklearn\n",
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset.\n",
        "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') # training data\n",
        "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # testing data\n",
        "print(dfeval.shape)\n",
        "y_train = dftrain.pop('survived')\n",
        "y_eval = dfeval.pop('survived')\n",
        "print(dftrain.loc[0], dftrain.loc[100]) # damit kann dann eine bestimmte Zeile im Datensatz ermittelt werden\n",
        "\n",
        "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n",
        "                       'embark_town', 'alone']\n",
        "NUMERIC_COLUMNS = ['age', 'fare']\n",
        "\n",
        "\n",
        "#Feature Collums\n",
        "#------------------------------------------------------------------------------------------------------------------------\n",
        "feature_columns = []\n",
        "for feature_name in CATEGORICAL_COLUMNS:\n",
        "  vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column; es werden alle einzigartigen werte Herausgenommen\n",
        "  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)) # fuegt alles in eine Zeile zusammen\n",
        "\n",
        "for feature_name in NUMERIC_COLUMNS:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
        "\n",
        "print(feature_columns)\n",
        "\n",
        "\n",
        "#Input Function\n",
        "#--------------------------------------------------------------------------------------------------------------------------\n",
        "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
        "  def input_function():  # inner function, this will be returned\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(1000)  # randomize order of data\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs\n",
        "    return ds  # return a batch of the dataset\n",
        "  return input_function  # return a function object for use\n",
        "\n",
        "train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model\n",
        "eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Creating the model\n",
        "#-----------------------------------------------------------------------------------------------------------------------------\n",
        "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\n",
        "# We create a linear estimtor by passing the feature columns we created earlier\n",
        "\n",
        "\n",
        "\n",
        "#Trining the model\n",
        "#------------------------------------------------------------------------------------------------------------------------------\n",
        "linear_est.train(train_input_fn)  # train\n",
        "result = linear_est.evaluate(eval_input_fn)  # get model metrics/stats by testing on tetsing data\n",
        "\n",
        "clear_output()  # clears consoke output\n",
        "print(result['accuracy'])  # the result variable is simply a dict of stats about our model\n",
        "print(result)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3vkJi2FF1VK"
      },
      "source": [
        "result = list(linear_est.predict(eval_input_fn))\n",
        "print(dfeval.loc[4])\n",
        "print(y_eval.loc[4])\n",
        "print(result[4][\"probabilities\"][1]) #[1] bedeutet das es die Wahrscheinlichkeit zum überleben ist; [0] bedeuted wahrscheinlnichkeit das sterben"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQEqyRCybzsv"
      },
      "source": [
        "**Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njsLtbNoNbzm"
      },
      "source": [
        "#Classification\n",
        "\n",
        "\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#Dataset\n",
        "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
        "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
        "# Lets define some constants to help us later on\n",
        "\n",
        "train_path = tf.keras.utils.get_file(\n",
        "    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n",
        "test_path = tf.keras.utils.get_file(\n",
        "    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n",
        "\n",
        "train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
        "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
        "# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe\n",
        "\n",
        "train_y = train.pop('Species')\n",
        "test_y = test.pop('Species')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Input function\n",
        "def input_fn(features, labels, training=True, batch_size=256):\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
        "\n",
        "    # Shuffle and repeat if you are in training mode.\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(1000).repeat()\n",
        "    \n",
        "    return dataset.batch(batch_size)\n",
        "\n",
        "#Feature Column\n",
        "my_feature_columns = []\n",
        "for key in train.keys():\n",
        "   my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
        "\n",
        "\n",
        "#Building the modul\n",
        "# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns=my_feature_columns,\n",
        "    # Two hidden layers of 30 and 10 nodes respectively.\n",
        "    hidden_units=[30, 10],\n",
        "    # The model must choose between 3 classes.\n",
        "    n_classes=3)\n",
        "\n",
        "\n",
        "\n",
        "#Traininig\n",
        "classifier.train(\n",
        "    input_fn=lambda: input_fn(train, train_y, training=True), # lambda funktion erstellt, im Anschluss steht, was die Funktion macht), Lambda Funktin ersetzt in der Input-Function die äußerer Fuktion??\n",
        "    steps=5000)\n",
        "# We include a lambda to avoid creating an inner function previously\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZMM71i_WJTV"
      },
      "source": [
        "eval_result = classifier.evaluate(input_fn=lambda: input_fn(test, test_y, training=False))\n",
        "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIPmmXWbWd6r"
      },
      "source": [
        "#Prediction\n",
        "def input_fn(features, batch_size=256):\n",
        "    # Convert the inputs to a Dataset without labels.\n",
        "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size) # gibt keine Labels  \n",
        "\n",
        "features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
        "predict = {} # hier kann ein Datensatz eingefuegt werden\n",
        "\n",
        "print(\"Please type numeric values as prompted.\")\n",
        "for feature in features:\n",
        "  valid = True\n",
        "  while valid: \n",
        "    val = input(feature + \": \")\n",
        "    if not val.isdigit(): valid = False\n",
        "\n",
        "  predict[feature] = [float(val)]   #bis hier werden alle features abgefragt und die Werte eingegeben, bis es keine Features mehr gibt\n",
        "\n",
        "predictions = classifier.predict(input_fn=lambda: input_fn(predict))\n",
        "for pred_dict in predictions:\n",
        "    print(pred_dict)\n",
        "    class_id = pred_dict['class_ids'][0]\n",
        "    probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "    print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
        "        SPECIES[class_id], 100 * probability))\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  # Here is some example input and expected classes you can try above\n",
        "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
        "predict_x = {\n",
        "    'SepalLength': [5.1, 5.9, 6.9],\n",
        "    'SepalWidth': [3.3, 3.0, 3.1],\n",
        "    'PetalLength': [1.7, 4.2, 5.4],\n",
        "    'PetalWidth': [0.5, 1.5, 2.1],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K7yWMGVYwe-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnEglvQ4b4w_"
      },
      "source": [
        "**Clustering**\n",
        "\n",
        "Bedeutet das mehrer Punkte zu einem Cluster zusammengefasst werden und das immer wieder gemacht wird\n",
        "-> so entstehen dann die Punkte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIRqlXdcuoCe"
      },
      "source": [
        "**Hidden Markov Models**\n",
        "\n",
        "Das Hidden-Markov-Modell ist eine endliche Menge von Zuständen, von denen jeder mit einer (im Allgemeinen mehrdimensionalen) Wahrscheinlichkeitsverteilung [] verbunden ist. Übergänge zwischen den Zuständen werden durch eine Menge von Wahrscheinlichkeiten geregelt, die Übergangswahrscheinlichkeiten genannt werden.\" \n",
        "\n",
        "Ein Hidden-Markov-Modell arbeitet mit Wahrscheinlichkeiten, um zukünftige Ereignisse oder Zustände vorherzusagen. In diesem Abschnitt erfahren Sie, wie Sie ein Hidden-Markov-Modell erstellen, das das Wetter vorhersagen kann.\n",
        "\n",
        "Zustände: In jedem Markov-Modell haben wir eine endliche Menge von Zuständen. Diese Zustände könnten etwa „warm“ und „kalt“ oder „hoch“ und „niedrig“ oder sogar „rot“, „grün“ und „blau“ sein. Diese Zustände sind im Modell \"versteckt\", das heißt, wir beobachten sie nicht direkt.\n",
        "\n",
        "Beobachtungen: Jedem Zustand ist basierend auf einer Wahrscheinlichkeitsverteilung ein bestimmtes Ergebnis oder eine bestimmte Beobachtung zugeordnet. Ein Beispiel dafür ist das folgende: An einem heißen Tag hat Tim eine Chance von 80 %, glücklich und eine Chance von 20 %, traurig zu sein.\n",
        "\n",
        "Übergänge: Jeder Zustand hat eine Wahrscheinlichkeit, die die Wahrscheinlichkeit des Übergangs in einen anderen Zustand definiert. Ein Beispiel ist das folgende: Auf einen kalten Tag kann mit einer Wahrscheinlichkeit von 30 % ein heißer Tag folgen und mit einer Wahrscheinlichkeit von 70 %, dass ein weiterer kalter Tag folgt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHt5GpmqwKwV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}